{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name: HRITIK BANA\n",
    "# Roll No.: 19141"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) On Policy TD control (SARSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import HRITIK_BANA_19141_warehouse\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HRITIK_BANA_19141_warehouse.WarehouseAgent()\n",
    "alpha = 0.85\n",
    "EPS = 0.92\n",
    "gamma = 0.95\n",
    "\n",
    "Q = {}\n",
    "xcoord = [i for i in range(7)]\n",
    "ycoord = [i for i in range(6)]\n",
    "\n",
    "actionSpace = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "\n",
    "stateSpace = [] \n",
    "\n",
    "for i in xcoord:\n",
    "    for j in ycoord:\n",
    "        for action in actionSpace:\n",
    "            Q[((i, j), action)] = 0\n",
    "        stateSpace.append((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose(state):\n",
    "    rand = np.random.random()\n",
    "    if rand < 1 - EPS:\n",
    "#         state = (x, y)\n",
    "        values = np.array([Q[(state, a)] for a in actionSpace])\n",
    "        best = np.random.choice(np.where(values == values.max())[0])\n",
    "        policy[state] = actionSpace[best]\n",
    "    else:\n",
    "        policy[state] = np.random.choice(actionSpace)\n",
    "    return policy[state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Best selected policy : {(0, 0): 'RIGHT', (0, 1): 'LEFT', (0, 2): 'DOWN', (0, 3): 'LEFT', (0, 4): 'UP', (0, 5): 'DOWN', (1, 0): 'UP', (1, 1): 'RIGHT', (1, 2): 'DOWN', (1, 3): 'LEFT', (1, 4): 'DOWN', (1, 5): 'DOWN', (2, 0): 'RIGHT', (2, 1): 'DOWN', (2, 2): 'DOWN', (2, 3): 'UP', (2, 4): 'LEFT', (2, 5): 'DOWN', (3, 0): 'LEFT', (3, 1): 'UP', (3, 2): 'LEFT', (3, 3): 'RIGHT', (3, 4): 'DOWN', (3, 5): 'UP', (4, 0): 'RIGHT', (4, 1): 'UP', (4, 2): 'UP', (4, 3): 'LEFT', (4, 4): 'LEFT', (4, 5): 'RIGHT', (5, 0): 'LEFT', (5, 1): 'UP', (5, 2): 'LEFT', (5, 3): 'LEFT', (5, 4): 'UP', (5, 5): 'RIGHT', (6, 0): 'RIGHT', (6, 1): 'UP', (6, 2): 'RIGHT', (6, 3): 'DOWN', (6, 4): 'RIGHT', (6, 5): 'DOWN'}\n"
     ]
    }
   ],
   "source": [
    "policy = {}\n",
    "for state in stateSpace:\n",
    "    policy[state] = np.random.choice(actionSpace)\n",
    "\n",
    "numEpisodes = 1000\n",
    "for i in range(numEpisodes):\n",
    "    statesActionsReturns = []\n",
    "    memory = []\n",
    "    observation, box_observation = env.reset()\n",
    "    done = False\n",
    "    count_steps = 0\n",
    "    max_steps = 50\n",
    "    while not done:\n",
    "        if count_steps == max_steps:\n",
    "            break\n",
    "        count_steps += 1\n",
    "        action = choose(tuple(observation))\n",
    "        observation_, reward, done = env.step(action)\n",
    "        memory.append((observation[0], observation[1], action, reward))\n",
    "        observation = observation_\n",
    "    memory.append((observation[0], observation[1], action, reward))\n",
    "\n",
    "    last = True\n",
    "    for x, y, action, reward in reversed(memory):\n",
    "        if last:\n",
    "            last = False\n",
    "        else:\n",
    "            statesActionsReturns.append((x, y, action, reward))\n",
    "\n",
    "\n",
    "    statesActionsReturns.reverse()\n",
    "    statesActionsVisited = []\n",
    "\n",
    "\n",
    "    x0, y0, action0, reward0 = statesActionsReturns[0]    \n",
    "    sa = ((x0, y0), action0)\n",
    "\n",
    "    for x, y, action, reward in statesActionsReturns:\n",
    "        sa_ = ((x, y), action)\n",
    "        if sa_ not in statesActionsVisited:\n",
    "            Q[sa] += alpha * (reward + (gamma * Q[sa_]) - Q[sa])\n",
    "            sa = sa_\n",
    "            statesActionsVisited.append(sa)\n",
    "\n",
    "    if EPS - 1e-7 > 0:\n",
    "        EPS -= 1e-7\n",
    "    else:\n",
    "        EPS = 0\n",
    "    \n",
    "    eps_returns.append(np.array(statesActionsReturns)[:, 3].astype(np.float).mean())\n",
    "\n",
    "print('\\n')\n",
    "print(\"Best selected policy :\", policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Off policy TD control (Q-Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HRITIK_BANA_19141_warehouse.WarehouseAgent()\n",
    "alpha = 0.85\n",
    "EPS = 0.92\n",
    "gamma = 0.95\n",
    "\n",
    "Q = {}\n",
    "xcoord = [i for i in range(7)]\n",
    "ycoord = [i for i in range(6)]\n",
    "\n",
    "actionSpace = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "\n",
    "stateSpace = [] \n",
    "\n",
    "for i in xcoord:\n",
    "    for j in ycoord:\n",
    "        for action in actionSpace:\n",
    "            Q[((i, j), action)] = 0\n",
    "        stateSpace.append((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose(state):\n",
    "    rand = np.random.random()\n",
    "    if rand < 1 - EPS:\n",
    "#         state = (x, y)\n",
    "        values = np.array([Q[(state, a)] for a in actionSpace])\n",
    "        best = np.random.choice(np.where(values == values.max())[0])\n",
    "        policy[state] = actionSpace[best]\n",
    "    else:\n",
    "        policy[state] = np.random.choice(actionSpace)\n",
    "    return policy[state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Best selected policy : {(0, 0): 'LEFT', (0, 1): 'RIGHT', (0, 2): 'LEFT', (0, 3): 'RIGHT', (0, 4): 'UP', (0, 5): 'RIGHT', (1, 0): 'LEFT', (1, 1): 'RIGHT', (1, 2): 'DOWN', (1, 3): 'UP', (1, 4): 'RIGHT', (1, 5): 'RIGHT', (2, 0): 'RIGHT', (2, 1): 'UP', (2, 2): 'DOWN', (2, 3): 'RIGHT', (2, 4): 'DOWN', (2, 5): 'LEFT', (3, 0): 'LEFT', (3, 1): 'RIGHT', (3, 2): 'DOWN', (3, 3): 'LEFT', (3, 4): 'DOWN', (3, 5): 'DOWN', (4, 0): 'DOWN', (4, 1): 'RIGHT', (4, 2): 'DOWN', (4, 3): 'LEFT', (4, 4): 'LEFT', (4, 5): 'LEFT', (5, 0): 'LEFT', (5, 1): 'UP', (5, 2): 'LEFT', (5, 3): 'LEFT', (5, 4): 'UP', (5, 5): 'LEFT', (6, 0): 'RIGHT', (6, 1): 'LEFT', (6, 2): 'RIGHT', (6, 3): 'LEFT', (6, 4): 'DOWN', (6, 5): 'LEFT'}\n"
     ]
    }
   ],
   "source": [
    "policy = {}\n",
    "for state in stateSpace:\n",
    "    policy[state] = np.random.choice(actionSpace)\n",
    "eps_returns = []\n",
    "numEpisodes = 1000\n",
    "for i in range(numEpisodes):\n",
    "    statesActionsReturns = []\n",
    "    memory = []\n",
    "    observation, box_observation = env.reset()\n",
    "    done = False\n",
    "    count_steps = 0\n",
    "    max_steps = 100\n",
    "    while not done:\n",
    "        if count_steps == max_steps:\n",
    "            break\n",
    "        count_steps += 1\n",
    "        action = choose(tuple(observation))\n",
    "        observation_, reward, done = env.step(action)\n",
    "        memory.append((observation[0], observation[1], action, reward))\n",
    "        observation = observation_\n",
    "    memory.append((observation[0], observation[1], action, reward))\n",
    "    \n",
    "    \n",
    "    last = True\n",
    "    for x, y, action, reward in reversed(memory):\n",
    "        if last:\n",
    "            last = False\n",
    "        else:\n",
    "            statesActionsReturns.append((x, y, action, reward))\n",
    "\n",
    "\n",
    "    statesActionsReturns.reverse()\n",
    "    statesActionsVisited = []\n",
    "\n",
    "\n",
    "    x0, y0, action0, reward0 = statesActionsReturns[0]    \n",
    "    sa = ((x0, y0), action0)\n",
    "\n",
    "    for x, y, action, reward in statesActionsReturns:\n",
    "        try:\n",
    "            \n",
    "            values = np.array([Q[((x,y), a)] for a in actionSpace])\n",
    "            a_ = np.where(values == values.max())[0][0]\n",
    "            sa_ = ((x,y), a_)\n",
    "            if sa_ not in statesActionsVisited:\n",
    "                Q[sa] += alpha * (reward + (gamma * Q[sa_]) - Q[sa])\n",
    "                sa = sa_\n",
    "                statesActionsVisited.append(sa)            \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    if EPS - 1e-7 > 0:\n",
    "        EPS -= 1e-7\n",
    "    else:\n",
    "        EPS = 0\n",
    "    eps_returns.append(np.array(statesActionsReturns)[:, 3].astype(np.float).mean())\n",
    "\n",
    "print('\\n')\n",
    "print(\"Best selected policy :\", policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20c1a1501c8>]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [i for i in range(numEpisodes)]\n",
    "\n",
    "plt.plot(x, eps_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) On Policy Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HRITIK_BANA_19141_warehouse.WarehouseAgent()\n",
    "EPS = 0.05\n",
    "GAMMA = 1.0\n",
    "\n",
    "Q = {}\n",
    "\n",
    "xcoord = [i for i in range(7)]\n",
    "\n",
    "ycoord = [i for i in range(6)]\n",
    "\n",
    "actionSpace = ['UP', 'DOWN', 'LEFT', 'RIGHT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateSpace = []\n",
    "returns = {}\n",
    "pairsVisited = {}\n",
    "\n",
    "\n",
    "for i in xcoord:\n",
    "    for j in ycoord:\n",
    "        for action in actionSpace:\n",
    "            Q[((i, j), action)] = 0\n",
    "            returns[((i, j), action)] = 0\n",
    "            pairsVisited[((i, j), action)] = 0\n",
    "        stateSpace.append((i, j))\n",
    "\n",
    "\n",
    "policy = {}\n",
    "for state in stateSpace:\n",
    "    policy[state] = np.random.choice(actionSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Best selected policy : {(0, 0): 'UP', (0, 1): 'RIGHT', (0, 2): 'RIGHT', (0, 3): 'DOWN', (0, 4): 'UP', (0, 5): 'UP', (1, 0): 'DOWN', (1, 1): 'DOWN', (1, 2): 'DOWN', (1, 3): 'DOWN', (1, 4): 'DOWN', (1, 5): 'DOWN', (2, 0): 'LEFT', (2, 1): 'RIGHT', (2, 2): 'RIGHT', (2, 3): 'LEFT', (2, 4): 'UP', (2, 5): 'UP', (3, 0): 'RIGHT', (3, 1): 'RIGHT', (3, 2): 'UP', (3, 3): 'UP', (3, 4): 'UP', (3, 5): 'LEFT', (4, 0): 'UP', (4, 1): 'UP', (4, 2): 'DOWN', (4, 3): 'DOWN', (4, 4): 'UP', (4, 5): 'DOWN', (5, 0): 'RIGHT', (5, 1): 'LEFT', (5, 2): 'DOWN', (5, 3): 'UP', (5, 4): 'RIGHT', (5, 5): 'RIGHT', (6, 0): 'RIGHT', (6, 1): 'RIGHT', (6, 2): 'DOWN', (6, 3): 'LEFT', (6, 4): 'LEFT', (6, 5): 'UP'}\n"
     ]
    }
   ],
   "source": [
    "numEpisodes = 1000\n",
    "for i in range(numEpisodes):\n",
    "    statesActionsReturns = []\n",
    "    memory = []\n",
    "    observation, box_observation = env.reset()\n",
    "    done = False\n",
    "    count_steps = 0\n",
    "    max_steps = 50\n",
    "    while not done:\n",
    "        if count_steps == max_steps:\n",
    "            break\n",
    "        count_steps += 1\n",
    "        action = policy[tuple(observation)]\n",
    "        observation_, reward, done = env.step(action)\n",
    "        memory.append((observation[0], observation[1], action, reward))\n",
    "        observation = observation_\n",
    "    memory.append((observation[0], observation[1], action, reward))\n",
    "\n",
    "    G = 0\n",
    "    last = True\n",
    "    for x, y, action, reward in reversed(memory):\n",
    "        if last:\n",
    "            last = False\n",
    "        else:\n",
    "            statesActionsReturns.append((x, y, action, G))\n",
    "        G = GAMMA*G + reward\n",
    "\n",
    "\n",
    "    statesActionsReturns.reverse()\n",
    "    statesActionsVisited = []\n",
    "\n",
    "\n",
    "    for x, y, action, G in statesActionsReturns:\n",
    "        sa = ((x, y), action)\n",
    "        if sa not in statesActionsVisited:\n",
    "            pairsVisited[sa] += 1\n",
    "\n",
    "            returns[(sa)] += (1 / pairsVisited[(sa)])*(G-returns[(sa)])\n",
    "            Q[sa] = returns[sa]\n",
    "            rand = np.random.random()\n",
    "            if rand < 1 - EPS:\n",
    "                state = (x, y)\n",
    "                values = np.array([Q[(state, a)] for a in actionSpace])\n",
    "                best = np.random.choice(np.where(values == values.max())[0])\n",
    "                policy[state] = actionSpace[best]\n",
    "            else:\n",
    "                policy[state] = np.random.choice(actionSpace)\n",
    "            statesActionsVisited.append(sa)\n",
    "\n",
    "    if EPS - 1e-7 > 0:\n",
    "        EPS -= 1e-7\n",
    "    else:\n",
    "        EPS = 0\n",
    "\n",
    "print('\\n')\n",
    "print(\"Best selected policy :\", policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Off-policy Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Best policy : {(0, 0): 'UP', (0, 1): 'UP', (0, 2): 'UP', (0, 3): 'UP', (0, 4): 'UP', (0, 5): 'RIGHT', (1, 0): 'UP', (1, 1): 'LEFT', (1, 2): 'LEFT', (1, 3): 'UP', (1, 4): 'DOWN', (1, 5): 'LEFT', (2, 0): 'DOWN', (2, 1): 'RIGHT', (2, 2): 'DOWN', (2, 3): 'UP', (2, 4): 'DOWN', (2, 5): 'LEFT', (3, 0): 'LEFT', (3, 1): 'LEFT', (3, 2): 'RIGHT', (3, 3): 'RIGHT', (3, 4): 'UP', (3, 5): 'UP', (4, 0): 'RIGHT', (4, 1): 'RIGHT', (4, 2): 'LEFT', (4, 3): 'LEFT', (4, 4): 'DOWN', (4, 5): 'UP', (5, 0): 'UP', (5, 1): 'LEFT', (5, 2): 'UP', (5, 3): 'LEFT', (5, 4): 'UP', (5, 5): 'DOWN', (6, 0): 'UP', (6, 1): 'RIGHT', (6, 2): 'DOWN', (6, 3): 'LEFT', (6, 4): 'LEFT', (6, 5): 'UP'}\n"
     ]
    }
   ],
   "source": [
    "env = HRITIK_BANA_19141_warehouse.WarehouseAgent()\n",
    "EPS = 0.05\n",
    "GAMMA = 1.0\n",
    "\n",
    "\n",
    "xcoord = [i for i in range(7)]\n",
    "\n",
    "ycoord = [i for i in range(6)]\n",
    "\n",
    "actionSpace = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "\n",
    "stateSpace = []\n",
    "returns = {}\n",
    "pairsVisited = {}\n",
    "\n",
    "Q = {}\n",
    "C = {}\n",
    "for i in xcoord:\n",
    "    for j in ycoord:\n",
    "        for action in actionSpace:\n",
    "            Q[((i, j), action)] = 0\n",
    "            C[((i, j), action)] = 0\n",
    "        stateSpace.append((i, j))\n",
    "\n",
    "\n",
    "targetPolicy = {}\n",
    "for state in stateSpace:\n",
    "    values = np.array([Q[(state, a)] for a in actionSpace])\n",
    "    best = np.random.choice(np.where(values==values.max())[0])\n",
    "    targetPolicy[state] = actionSpace[best]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "numEpisodes = 1000\n",
    "for i in range(numEpisodes):\n",
    "    memory = []\n",
    "    behaviourPolicy = {}\n",
    "    for state in stateSpace:\n",
    "        rand = np.random.random()\n",
    "        if rand < 1 - EPS:\n",
    "            behaviourPolicy[state] = [targetPolicy[state]]\n",
    "        else:\n",
    "            behaviourPolicy[state] = actionSpace\n",
    "\n",
    "    observation, box_observation = env.reset()\n",
    "    done = False\n",
    "    count_steps = 0\n",
    "    max_steps = 1000\n",
    "    while not done:\n",
    "        if count_steps == max_steps:\n",
    "            break\n",
    "        count_steps += 1\n",
    "        action = np.random.choice(behaviourPolicy[tuple(observation)])\n",
    "        observation_, reward, done = env.step(action)\n",
    "        memory.append((observation[0], observation[1], action, reward))\n",
    "        observation = observation_\n",
    "    memory.append((observation[0], observation[1], action, reward))\n",
    "\n",
    "    G = 0\n",
    "    W = 1\n",
    "    last = True\n",
    "    for x, y, action, reward in reversed(memory):\n",
    "        sa = ((x,y), action)\n",
    "        if last:\n",
    "            last = False\n",
    "        else:\n",
    "            C[sa] += W\n",
    "            Q[sa] += (W / C[sa])*(G - Q[sa])\n",
    "            values = np.array([Q[(state, a)] for a in actionSpace])\n",
    "            best = np.random.choice(np.where(values == values.max())[0])\n",
    "            targetPolicy[state] = actionSpace[best]\n",
    "            if action != targetPolicy[state]:\n",
    "                break\n",
    "            if len(behaviourPolicy[state]) == 1:\n",
    "                prob = 1 - EPS\n",
    "            else:\n",
    "                prob = EPS / len(behaviourPolicy[state])\n",
    "            W *= 1/prob\n",
    "        G = GAMMA*G + reward\n",
    "   \n",
    "    if EPS - 1e-7 > 0:\n",
    "        EPS -= 1e-7\n",
    "    else:\n",
    "        EPS = 0\n",
    "\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print(\"Best policy :\", targetPolicy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On starting with the given initial conditions, the agent should ideally follow a path like: DOWN, DOWN, RIGHT, RIGHT, DOWN, LEFT, LEFT, DOWN, LEFT, UP. Looking at the best policy obtained by different methods, SARSA and Q-Learning, mostly follow the above path in the corresponding agent locations. So it seems that SARSA and Q-Learning are performing the best with respect to this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
